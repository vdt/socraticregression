<!DOCTYPE HTML>
<html lang="en">
  <head>
    <link rel="stylesheet" href="css/main.css">
    <link rel="shortcut icon" href="img/favicon.ico" type="image/x-icon" />
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville|    Roboto:700,500,300|Roboto+Condensed">
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <title>Socratic Regression</title>
  </head>
  <body>
    <div id="column">
      <div class="header">
        <a href="/"><img class="logo" src="img/socrates3.png"></a>
        <div class="wording">
          <a href="/"><div class="title">Socratic Regression</div></a>
          <div class="caption">A Geometric Interpretation of Linear Regression</div>
        </div>
      </div>
      <div class="content">
        <h3 id="whats-socratic-regression">What’s Socratic Regression?</h3>

<p>It is an attempt to develop an intuitive understanding of linear regression using Socratic questioning. The starting point for this project was a <a href="https://web.archive.org/web/20040213071852/http://emlab.berkeley.edu/GMTheorem/index.html">geometric proof of the Gauss-Markov theorem</a> which appealed to me as a visual learner. While examining it, I found myself running through lines of questioning in my head which made me realize that Socratic questioning is a more natural way to learn the subject than the traditional ground-up, proof-based approach.</p>

<h3 id="whats-this-gauss-markov-theorem-you-speak-of-how-is-it-relevant-to-linear-regression">What’s this Gauss-Markov theorem you speak of? How is it relevant to linear regression?</h3>

<p>The Gauss-Markov theorem is a good jump-off point from which to learn about linear regression. There are many ways to fit a line through data, of which ordinary least squares (OLS) is the most well-known. (What! I didn’t realize there are other ways to fit a line?! What are some examples? Generalized least squares (GLS), maximum likelihood estimation (MLE), etc.) The Gauss-Markov theorem explains that under certain conditions, OLS is the “best” method to use.</p>

<p>This is interesting in two ways. First, it helps us understand why everyone seems to like using OLS when doing linear regression. Second, like a big fish in a small pond, OLS is only best in a very narrow sense — it requires several strong conditions and is only best in a small class of methods. By tearing down the various conditions and probing why it can’t compete outside of its class, we end up with a good understanding of linear regression.</p>

<h3 id="so-what-does-it-mean-for-ols-to-be-best-and-what-the-conditions-are-required-for-this">So what does it mean for OLS to be “best”? And what the conditions are required for this?</h3>

<p>Recall, the problem of linear regression says: Suppose we have data that is generated in the secret control room of the universe** by the function <script type="math/tex">X\beta + \epsilon</script>. What is the best way to find <script type="math/tex">\beta</script>? Remember that as mere mortals, we can never peek into the secret control room of the universe (SCRU). We can only observe <script type="math/tex">y</script>, final result of <script type="math/tex">X\beta + \epsilon</script>, and <script type="math/tex">X</script> which are our inputs. <script type="math/tex">\beta</script> is top secret so we can’t just read it off our observations, and <script type="math/tex">\epsilon</script> is a random variable which prevents us from deducing \beta from what we know. We are forced to guess what it is and this process of guessing parameters of underlying models is known in statistics as “estimation”.</p>

<p>The Gauss-Markov theorem says: This problem is way too hard. I can’t tell you the best estimator for <script type="math/tex">\beta</script> in all cases; but for the cases where the noise <script type="math/tex">\epsilon</script> is distributed with <script type="math/tex">\mathbb{E}(\epsilon) = 0</script> and <script type="math/tex">\mathbb{Var}(\epsilon) = \sigma^2 I</script>, then I can tell you that OLS is the best among the class of <em>linear</em>, <em>unbiased</em> estimators.</p>

<p>[Some people familiar with regression may ask: Don’t we also need the condition that the data is correctly modeled by <script type="math/tex">X\beta</script>? In my view, this condition is unnecessary because it is part of the setup of the problem.]</p>

<p>——</p>

<p>We start with a 3 dimensional space with a plane in it. The plane represents the <script type="math/tex">X</script> matrix.</p>

<p><img src="img/xplane.png" /></p>

<h3 id="what-is-x-and-why-is-it-represented-by-a-plane">What is <script type="math/tex">X</script> and why is it represented by a plane?</h3>

<p>One superficial view of the <script type="math/tex">X</script> matrix is as a store of data where each row stores the covariates of a single data point. <script type="math/tex">X</script> is tall and skinny because it must have at least as many data points as it has covariates. This is reminiscent of what we know from high school algebra about systems of equations. Each equations provides information about the relationship between the inputs and outputs, and if we have more unknowns than we have equations, we won’t have enough information to find a unique solution for the unknowns.</p>

<p>[diagram showing shapes of y = X\beta]</p>

<p>We can also view <script type="math/tex">X</script> as a linear transformation since we can feed it a vector and have it spit out another. More concretely, it takes in a <script type="math/tex">\beta</script> vector and uses the vector’s components as weights for building a linear combination of its columns. This linear combination is then output as the vector of predictions <script type="math/tex">y</script>. I like to think of <script type="math/tex">X</script> as a fridge where each column is an ingredient. <script type="math/tex">\beta</script> is a recipe which tells us how much of each ingredient to mix together.</p>

<script type="math/tex; mode=display">y = X \beta = \beta_1 \begin{bmatrix} | \\ x_1 \\ | \end{bmatrix} + \beta_2 \begin{bmatrix} | \\ x_2 \\ | \end{bmatrix} + … + \beta_i \begin{bmatrix} | \\ x_i \\ | \end{bmatrix}</script>

<p>This leads us to a third geometric view of <script type="math/tex">X</script>. Since <script type="math/tex">X</script> is a transformation, we can visualize it as the space of all its possible outputs. At first you might wonder: There are so many possible outputs of X. How do I visualize this? It turns out that linear algebra has good tools for this. Recall that the space of all possible outputs is the space of all possible linear combinations of X’s columns. In the language of linear algebra, we call this the space <em>spanned</em> by the column vectors, or the column space for short.</p>

<p><img src="img/xspan.png" /></p>

<p>Since <script type="math/tex">X</script> is tall and skinny, it has a few tall column vectors. This tells us that the column vectors live in a high-dimensional space and that there are not enough of them to span it. Therefore, the column space is a subspace of the main space. Since we’re limited to imagining the main space in 3 dimensions, the column space of <script type="math/tex">X</script> is best represented by a 2 dimensional subspace, a plane.</p>

<h3 id="alright-so-we-have-this-plane-how-does-the-problem-of-linear-regression-look-like-in-this-picture">Alright, so we have this plane. How does the problem of linear regression look like in this picture?</h3>

<p>Recall that the <script type="math/tex">X</script> plane consists of all possible outputs of <script type="math/tex">X</script> including the output of the One True <script type="math/tex">\beta</script>. This <script type="math/tex">\beta</script> was generated in the SCRU so we can’t observe the vector it produces. What we do observe comes one step later after obscuring noise, <script type="math/tex">\epsilon</script>, is added.</p>

<p><img src="img/yobserve.png" /></p>

<p>The problem of linear regression is to estimate a <script type="math/tex">\hat{\beta}</script> which produces an <script type="math/tex">X\hat{\beta}</script> as close as possible to <script type="math/tex">X\beta</script>, based on the <script type="math/tex">y</script> that we observe. (<script type="math/tex">\hat{\beta}</script> is pronounced “beta hat” and is the standard notation to show that it is an estimator of a SCRU value.)</p>

<p><img src="img/bhats.png" /></p>

<p>That’s not all. This picture is incomplete because it only shows one observation of <script type="math/tex">y</script>. Remember that <script type="math/tex">y</script> consists of the pure, consummate <script type="math/tex">X\beta</script> combined with a chaotic <script type="math/tex">\epsilon</script>. Since <script type="math/tex">\epsilon</script> is a random variable, each time we observe <script type="math/tex">y</script>, it gives a different value. We don’t want to find a method of estimation that only works well on a lucky realization of <script type="math/tex">\epsilon</script>, we want it to work well on aggregate.</p>

<p>The solution is to consider the entire distribution of <script type="math/tex">\epsilon</script>. Each point in this cloud is a possible observation of <script type="math/tex">y</script>. Here, we’ve modeled <script type="math/tex">\epsilon</script> as a 3 dimension gaussian.</p>

<p><img src="img/cloud.png" /></p>

<p>This is still not good enough. Clouds are not great to reason with because they are irregular and lack nice boundaries. Instead, let’s abstract the randomness away by using confidence regions. The sphere below is essentially a 3 dimensional confidence interval where the <script type="math/tex">y</script>s are likely to be found. Also note that the region doesn’t have to be a sphere. It could be whatever shape specified by the distribution of <script type="math/tex">\epsilon</script>.</p>

<p><img src="img/sphere.png" /></p>

<p>When we were working with a single observation of <script type="math/tex">y</script>, the problem of linear regression was just about mapping it onto the plane, producing a <script type="math/tex">X\hat{\beta}</script> which lies as near to <script type="math/tex">X\beta</script> as possible. Now that we are working with a region of <script type="math/tex">y</script>s, we can restate the problem more generally as finding a way to map the confidence region on to the plane so that the resulting area of <script type="math/tex">X\hat{\beta}</script> is as closely bunched around <script type="math/tex">X\beta</script> as possible.</p>

<h3 id="presumably-ols-is-one-of-the-ways-to-estimate-a-hatbeta-how-does-does-the-ols-estimate-look-like">Presumably, OLS is one of the ways to estimate a <script type="math/tex">\hat{\beta}</script>. How does does the OLS estimate look like?</h3>

<p>Geometrically, the OLS estimate is the orthogonal projection of y onto to the X plane.</p>

<p><img src="img/ols.png" /></p>

<p>Again, the more realistic picture is the orthogonal projection of the y cloud onto the X plane.</p>

<p><img src="img/sphereols.png" /></p>

<h3 id="okay-but-why-does-ols-result-in-an-orthogonal-projection">Okay, but why does OLS result in an orthogonal projection?</h3>

<p>Recall the definition of the ordinary least squares estimator <script type="math/tex">\hat{\beta}_{OLS}</script>:</p>

<script type="math/tex; mode=display">\hat{\beta}_{OLS} = argmin_{\text{all possible }\hat{\beta}s} S(\hat{\beta})</script>

<p>where <script type="math/tex">S</script> is the sum of squares distance away from the observed <script type="math/tex">y</script>, that is</p>

<script type="math/tex; mode=display">S(\hat{\beta}) = \sum_i^n (y - x_i^T\hat{\beta})^2 = (y -X\hat{\beta})^T(y -X\hat{\beta})</script>

<p>We can sketch out contour lines around y that maintain the same sum of squares distances from <script type="math/tex">y</script>. These points form a circle because the definition of a circle, <script type="math/tex">x^2 + y^2 = constant</script>, is exactly the points that hold sum of squares constant. The smallest circle to touch the <script type="math/tex">X</script> plane touches it a tangent. At this point, both the curve and the <script type="math/tex">X</script> plane have the same gradient, so the normal line to the plane must point where the normal line of the curve is pointing, which is directly at <script type="math/tex">y</script> in the center of the circle.</p>

<p><img src="img/tangent.png" /></p>

<h3 id="now-we-know-how-the-ols-estimator-looks-like-geometrically-why-does-gauss-markov-say-it-is-best">Now we know how the OLS estimator looks like geometrically. Why does Gauss-Markov say it is best?</h3>

<p>The metric that the Gauss-Markov theorem uses to evaluate estimators is mean squared error. As with OLS, the squared error is the standard Euclidean distance we know and love. But instead of simply going through each of the possible values of <script type="math/tex">X\hat{\beta}_{OLS}</script> in the projected area and summing their distances from the One True <script type="math/tex">X\beta</script>, we have to find the mean and that requires us to weigh each distance by the probability of obtaining that value of <script type="math/tex">X\hat{\beta}_{OLS}</script>.</p>

<p>Visualizing this metric is difficult because of the probabilty weights. The most naive approach using size of the area isn’t good enough because it neglects the probabilities. Instead, we can think of the metric as rotational mass, or what physicists call “moment of inertia”. Imagine the <script type="math/tex">X\hat{\beta}</script>s as little unit volumes, their probabilities as their mass densities, and their distance from <script type="math/tex">X\beta</script> as their radius of rotation. A good area under the Gauss-Markov theorem, would thus have a small moment of inertia when spun about <script type="math/tex">X\beta</script>.</p>

<p>The Gauss-Markov theorem says that when the noise is distributed as a sphere centered around <script type="math/tex">X\beta</script>, the orthogonal projection (the OLS estimator) gives an area with the smallest moment of inertia.</p>

<p>We can get a sense for why this is true by looking at a non-orthogonal projection. The diagram shows that the sphere is projected onto an ellipse which has a higher moment of intertia than the circle.</p>

<p><img src="img/badprojection.png" /></p>

<h3 id="i-noticed-that-the-formal-conditions-of-gauss-markov-are-that-mathbbeepsilon--0-and-mathbbvarepsilon--sigma2-i-how-does-this-translate-into-a-sphere-centered-on-xbeta">I noticed that the formal conditions of Gauss-Markov are that <script type="math/tex">\mathbb{E}(\epsilon) = 0</script> and <script type="math/tex">\mathbb{Var}(\epsilon) = \sigma^2 I</script>. How does this translate into a sphere centered on <script type="math/tex">X\beta</script>?</h3>

<p>Continuing the idea of expectations as masses, <script type="math/tex">\mathbb{E}(\epsilon) = 0</script> tells us that the mass of the epsilons are centered at 0. Since <script type="math/tex">y = X\beta + \epsilon</script>, it means that the <script type="math/tex">y</script> cloud is a sphere centered at <script type="math/tex">X\beta</script>.</p>

<p><script type="math/tex">\mathbb{Var}(\epsilon) = \sigma^2 I</script> is a standard covariance matrix and it describes the shape of the cloud. The diagonal elements tell us the variance in one dimension, since all the of them are <script type="math/tex">\sigma^2</script>, we know that the variation of the cloud along each of the axes is the same. The off-diagonal terms tell us if there is extra stretching between axes. Here they are null, so we know that the variation is the same in all directions which is exactly what a sphere is.</p>

<p>Anatomy of a covariance matrix:</p>

<script type="math/tex; mode=display">% <![CDATA[
\mathbb{Var}(Z) =
\begin{bmatrix}
    Var(z_1) & Cov(z_1, z_2)  & \dots  & Cov(z_1, z_n) \\
    Cov(z_2, z_1) & Var(z_2)  & \dots  & Cov(z_2, z_n) \\
    \vdots & \vdots & \ddots & \vdots \\
    Cov(z_n, z_1) & Cov(z_n, z_2) & \dots  & Var(z_n)
\end{bmatrix} %]]></script>

<h3 id="do-we-know-why-gauss-markov-requires-the-centered-sphere-assumption-why-does-the-theorem-break-down-without-it">Do we know why Gauss-Markov requires the centered sphere assumption? Why does the theorem break down without it?</h3>

<p>This is where we can use this geometric environment to explore the Gauss-Markov theorem. Say we were to break the centering assumption, that <script type="math/tex">\mathbb{E}(\epsilon) = 0</script>. The <script type="math/tex">\epsilon</script> and it’s projection would no longer be centered around <script type="math/tex">X\beta</script>. One could conceive of a non-orthogonal projection that casts the sphere’s shadow back around <script type="math/tex">X\beta</script> resulting in a lower moment of inertia about <script type="math/tex">X\beta</script>.</p>

<p>(Talk about fixing by retilting X?)</p>

<p>We can break the spherical assumption by supposing <script type="math/tex">\epsilon</script> to be an ellipsoid. The orthogonal projection results in an ellipse. But if we can imagine how a non-orthogonal projection that is lined up with the major axis of the ellipsoid can cast a smaller area around <script type="math/tex">X\beta</script>, again resulting in lower moment of inertia.</p>

<p>In both of these cases, we see that the OLS orthogonal projection is no longer the best.</p>

<h3 id="we-understand-the-centered-sphere-assumption-is-necessary-what-about-the-other-constraints-placed-by-gauss-markov-why-cant-ols-compete-with-non-linear-or-biased-estimators">We understand the centered sphere assumption is necessary. What about the other constraints placed by Gauss-Markov? Why can’t OLS compete with non-linear or biased estimators?</h3>


      </div>
      <div class="footer">
        This work is licensed under a <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/">Creative Commons License</a> that allows sharing and adapting for non-commercial purposes, but requires attribution.
      </div>
    </div>
  </body>
</html>
